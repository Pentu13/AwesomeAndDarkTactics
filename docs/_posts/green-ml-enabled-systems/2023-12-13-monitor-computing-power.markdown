---
layout: tactic
title: Monitor Computing Power
tags:
  - machine-learning
  - management
t-sort: Awesome Tactic
t-type: Architectural Tactic
categories:
  - green-ml-enabled-systems
t-description: >-
  Estimating and calculating the energy footprint of a machine learning model
  can help to reduce the computational power of ML models. Monitoring the energy
  consumption of a ML model in the long term helps to identify those components
  where energy is being inefficiently utilized. This can serve as a starting
  point for making improvements to reduce energy consumption. There has been a
  lack of easy-to-use tools to do that, but recently  researchers have provided
  frameworks for how to estimate or calculate the energy footprint of machine
  learning.
t-participant: Data Scientist
t-artifact: Machine Learning Model
t-context: General
t-feature: <Unavailable>
t-intent: >-
  Improve energy efficiency by monitoring computing power of machine learning in
  the long term
t-targetQA: Energy Efficiency
t-relatedQA: <Unavailable>
t-measuredimpact: <Unavailable>
t-source: >-
  Qingqing Cao, Yash Kumar Lal, Harsh Trivedi, Aruna Balasubramanian, and
  Niranjan Balasubramanian. 2021. IrEne: Interpretable Energy Prediction for
  Transformers. Proceedings of the 59th Annual Meeting of the Association for
  Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing (Volume 1: Long Papers) (2021).
  [DOI](https://doi.org/10.48550/arXiv.2106.01199); Mohit Kumar, Xingzhou Zhang,
  Liangkai Liu, Yifan Wang, and Weisong Shi. 2020. Energy-Efficient Machine
  Learning on the Edges. In 2020 IEEE International Parallel and Distributed
  Processing Symposium Workshops (IPDPSW). 912â€“921.
  [DOI](https://doi.org/10.1109/IPDPSW50202.2020.00153)
t-source-doi: <Unavailable>
t-diagram: monitor-computing-power.png
t-intentmeasure: <Unavailable>
t-countermeasure: <Unavailable>
---

