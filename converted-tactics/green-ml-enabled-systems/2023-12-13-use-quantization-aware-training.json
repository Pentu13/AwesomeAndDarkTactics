{
  "layout": "tactic",
  "title": "Use Quantization-Aware Training",
  "tags": [
    "machine-learning",
    "model-training"
  ],
  "t-sort": "Awesome Tactic",
  "t-type": "Architectural Tactic",
  "categories": "green-ml-enabled-systems",
  "t-description": "Quantization-aware training is a technique used to train neural networks to convert data types to lower precision. The idea is to use fixed-point or integer representations instead of the more commonly used higher-precision floating-point representations. This improves the performance and energy efficiency of the model in federated learning.",
  "t-participant": "Data Scientist",
  "t-artifact": "Model",
  "t-context": "Machine Learning",
  "t-feature": "Model Training",
  "t-intent": "Improve energy efficiency by using quantization-aware training to convert high-precision data types to lower precision",
  "t-targetQA": "Accuracy",
  "t-relatedQA": "Energy Efficiency",
  "t-measuredimpact": "",
  "t-source": "Minsu Kim, Walid Saad, Mohammad Mozaffari, and Merouane Debbah. 2021. On the Tradeoff between Energy, Precision, and Accuracy in Federated Quantized Neural Networks. In ICC 2022 - IEEE International Conference on Communications. 2194â€“2199. [DOI](https://doi.org/10.1109/ICC45855.2022.9838362); Martino Sorbaro, Qian Liu, Massimo Bortone, and Sadique Sheik. 2020. Optimizing the Energy Consumption of Spiking Neural Networks for Neuromorphic Applications. Frontiers in Neuroscience 14 (2020), 662. [DOI](https://doi.org/10.3389/fnins.2020.00662)",
  "t-source-doi": "",
  "t-diagram": "use-quantization-aware-training.png",
  "t-intentmeasure": "",
  "t-countermeasure": ""
}
