{
  "layout": "tactic",
  "title": "Set Energy Consumption as a Model Constraint",
  "tags": [
    "machine-learning",
    "model-optimization"
  ],
  "t-sort": "Awesome Tactic",
  "t-type": "Architectural Tactic",
  "categories": "green-ml-enabled-systems",
  "t-description": "This tactic concerns setting a predetermined energy consumption threshold for the ML model optimization process. The optimization considers the energy consumption of the model during both the optimization and training phases. The objective is to train the model in a way that it stays within the specified energy consumption threshold. This approach views model optimization as an optimization problem, where for instance hyperparameters and the model itself are optimized based on the predetermined limits.",
  "t-participant": "Data Scientist",
  "t-artifact": "Machine Learning Algorithm",
  "t-context": "Machine Learning",
  "t-feature": "",
  "t-intent": "Improve energy efficiency by setting energy consumption as a constraint such that the model will stay below the threshold during training",
  "t-targetQA": "Energy Efficiency",
  "t-relatedQA": "Accuracy, Performance",
  "t-measuredimpact": "",
  "t-source": "Qu Wang, Yong Xiao, Huixiang Zhu, Zijian Sun, Yingyu Li, and Xiaohu Ge. 2021. Towards Energy-Efficient Federated Edge Intelligence for IoT Networks. In 2021 IEEE 41st International Conference on Distributed Computing Systems Workshops. [DOI](https://doi.org/10.1109/ICDCSW53096.2021.00016); Haichuan Yang, Yuhao Zhu, and Ji Liu. 2019. Energy-Constrained Compression for Deep Neural Networks Via Weighted Sparse Projection and Layer Input Masking. International Conference on Learning Representations (ICLR) (2019) (ICDCSW). 55â€“62. [DOI](https://doi.org/10.48550/arXiv.1806.04321)",
  "t-source-doi": "",
  "t-diagram": "set-energy-consumption-as-a-constraint.png",
  "t-intentmeasure": "",
  "t-countermeasure": ""
}
